{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LitAlign","text":"<p>Lightning-native library for fine-tuning LLMs using alignment techniques like PPO, DPO, GRPO and more.</p>"},{"location":"01-kl-divergence/","title":"\ud83d\udcd8 KL Divergence: Quick Notes","text":"<p>Kullback\u2013Leibler (KL) divergence</p> <p>The KL divergence between two probability distributions \\(P\\) and \\(Q\\) is:</p> \\[ D_{\\text{KL}}(P || Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\] <p>In the context of RL:</p> <ul> <li>\\(P\\): New (current) policy \\(\\pi\\_\\theta\\)</li> <li>\\(Q\\): Old (reference) policy \\(\\pi\\_{\\theta\\_{\\text{old}}}\\)</li> <li>Used to penalize divergence from a reference policy</li> </ul> <ul> <li>KL divergence yields how different two probability distributions are.</li> <li>Or, how much information is lost when \\(Q\\) is used to approximate \\(P\\).</li> </ul> <p>Note</p> <p>KL divergence is not symmetric: \\(D\\_{\\text{KL}}(P || Q) \\neq D\\_{\\text{KL}}(Q || P)\\). It measures how much information is lost when using \\(Q\\) to approximate \\(P\\). And vice versa.</p> <p>Important (GROK says:)</p> <p>In the expression ( D_{KL}(P||Q) ), the Kullback-Leibler (KL) divergence measures how much the probability distribution ( P ) diverges from the probability distribution ( Q ). Here's the breakdown:</p> <ul> <li>\\( P \\): This is the \"true\" or \"target\" distribution, the one you consider as the reference or the actual distribution you want to approximate.</li> <li>\\( Q \\): This is the \"approximating\" distribution, the one you use to estimate or approximate \\( P \\).</li> </ul>"},{"location":"01-kl-divergence/#interpretation","title":"Interpretation","text":"<ul> <li>\\( D_{KL}(P||Q) \\) quantifies the information loss when \\( Q \\) is used to approximate \\( P \\).</li> <li>It is not symmetric, meaning \\( D_{KL}(P||Q) \\neq D_{KL}(Q||P) \\), because the roles of the \"true\" and \"approximating\" distributions are not interchangeable.</li> </ul>"},{"location":"01-kl-divergence/#formula","title":"Formula","text":"<p>The KL divergence is defined as: $$ [ D_{KL}(P||Q) = \\sum_x P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right) ] $$ (for discrete distributions), or $$ [ D_{KL}(P||Q) = \\int P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right) dx ] $$ (for continuous distributions).</p>"},{"location":"01-kl-divergence/#key-points","title":"Key Points","text":"<ul> <li>\\( P \\) is the distribution you assume to be the true one.</li> <li>\\( Q \\) is the distribution you use to model or estimate \\( P \\).</li> <li>The asymmetry arises because \\( P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right) \\) weighs the log-ratio by \\( P(x) \\), not \\( Q(x) \\), so swapping them changes the result.</li> </ul> <p>So, in \\( D_{KL}(P||Q) \\), \\( Q \\) is used to estimate \\( P \\).</p>"},{"location":"01-kl-divergence/#pytorch-implementation","title":"\u2705 PyTorch Implementation","text":"<p>Assuming your policies are represented as <code>Categorical</code> distributions (e.g., action logits):</p> <ul> <li><code>kl_divergence.py</code></li> </ul> <pre><code>import torch\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\ndef kl_divergence_logits(logits_p: torch.Tensor, logits_q: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes KL divergence D_KL(P || Q) between two categorical distributions P and Q\n    given their logits. Shape: [batch_size, num_actions]\n\n    Args:\n        logits_p: Logits of new/current policy \u03c0\u03b8\n        logits_q: Logits of old/reference policy \u03c0\u03b8_old\n\n    Returns:\n        kl: Tensor of shape [batch_size] with KL divergence for each sample\n    \"\"\"\n    p = F.log_softmax(logits_p, dim=-1)\n    q = F.log_softmax(logits_q, dim=-1)\n\n    p_prob = p.exp()\n    kl = (p_prob * (p - q)).sum(dim=-1)\n    return kl\n</code></pre>"},{"location":"01-kl-divergence/#example-usage","title":"\ud83d\udd0d Example Usage","text":"<pre><code>batch_size = 4\nnum_actions = 3\n\nlogits_new = torch.randn(batch_size, num_actions)\nlogits_old = logits_new + 0.1 * torch.randn(batch_size, num_actions)  # small shift\n\nkl = kl_divergence_logits(logits_new, logits_old)\nprint(\"KL divergence per sample:\", kl)\n</code></pre>"},{"location":"01-kl-divergence/#for-gaussian-continuous-action-policies","title":"\u2699\ufe0f For Gaussian (Continuous Action) Policies","text":"<p>If you're using a Gaussian policy (e.g., in continuous control with mean &amp; std):</p> <pre><code>from torch.distributions import Normal, kl_divergence\n\ndef kl_gaussian(mean_p, std_p, mean_q, std_q):\n    dist_p = Normal(mean_p, std_p)\n    dist_q = Normal(mean_q, std_q)\n    return kl_divergence(dist_p, dist_q).sum(-1)  # Sum over action dims\n</code></pre>"},{"location":"01-kl-divergence/#where-you-use-this","title":"\ud83e\udde0 Where You Use This","text":"<ul> <li>PPO: <code>loss = policy_loss - \u03b2 * KL(...)</code></li> <li>DPO/GRPO: KL shows up in the policy regularizer</li> <li>TRPO: Uses KL as a trust region constraint</li> </ul>"},{"location":"02-entropy/","title":"\ud83e\udde0 Entropy in Reinforcement Learning","text":""},{"location":"02-entropy/#intuition","title":"\ud83d\ude80 Intuition","text":"<ul> <li> <p>Probability tells us how certain an agent is about taking an action.</p> </li> <li> <p>Surprise captures how unexpected an outcome is.</p> </li> <li> <p>So, a natural idea is:</p> </li> </ul> <p>$$   \\text{Surprise}(a) = \\frac{1}{P(a)}   $$</p> <p>But this isn't ideal:</p> <ul> <li>If \\(P(a) = 1\\), surprise should be 0 \u2014 but \\(\\frac{1}{1} = 1\\), which doesn\u2019t work.</li> <li>Instead, we define surprise using logarithms:</li> </ul>"},{"location":"02-entropy/#surprise-log-inverse-probability","title":"\ud83d\udcd0 Surprise = Log Inverse Probability","text":"\\[ \\text{Surprise}(a) = \\log\\left(\\frac{1}{P(a)}\\right) = -\\log P(a) \\] <p>So, the less likely the action, the greater the surprise.</p>"},{"location":"02-entropy/#entropy-expected-surprise","title":"\ud83d\udcca Entropy = Expected Surprise","text":"<p>Entropy is the expected surprise over all possible actions:</p> \\[ \\begin{align*} \\text{Entropy}(\\pi) &amp;= \\sum_{a \\in \\mathcal{A}} P(a) \\cdot \\text{Surprise}(a) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} P(a) \\cdot (-\\log P(a)) \\\\ &amp;= -\\sum_{a \\in \\mathcal{A}} P(a) \\cdot \\log P(a) \\end{align*} \\]"},{"location":"02-entropy/#what-entropy-tells-us","title":"\ud83d\udd01 What Entropy Tells Us","text":"Distribution Entropy Notes [1.0, 0.0, 0.0] 0 Fully deterministic [0.7, 0.2, 0.1] Low Fairly confident [0.33, 0.33, 0.34] High Very uncertain (max entropy) <p>\ud83d\udccc Entropy is highest when all actions are equally likely (pure exploration), and lowest when the policy is deterministic (pure exploitation).</p>"},{"location":"02-entropy/#pytorch-compute-entropy","title":"\ud83e\uddea PyTorch: Compute Entropy","text":"<p>If you have a probability distribution (e.g. from <code>softmax</code>), you can compute entropy like this:</p> <pre><code>import torch\nimport torch.nn.functional as F\n\n# Example: logits for 3 actions\nlogits = torch.tensor([1.0, 0.5, -0.5])\n\n# Get action probabilities\nprobs = F.softmax(logits, dim=-1)\n\n# Compute entropy\nentropy = -torch.sum(probs * torch.log(probs + 1e-8))  # +1e-8 for numerical stability\n\nprint(\"Entropy:\", entropy.item())\n</code></pre> <ul> <li>or, using <code>Categorical</code> distribution:</li> </ul> <pre><code>import torch\n\n# Example logits for a single state with 3 actions\nlogits = torch.tensor([1.0, 0.5, -0.5])\n\n# Create a categorical distribution\ndist = torch.distributions.Categorical(logits=logits)\n\n# Compute entropy\nentropy = dist.entropy()\n\nprint(\"Entropy:\", entropy.item())\n</code></pre>"},{"location":"02-entropy/#when-is-entropy-used-in-rl","title":"\u2699\ufe0f When is Entropy Used in RL?","text":"<ul> <li> <p>Policy Gradient Methods (PPO, A2C, etc.):</p> </li> <li> <p>Add entropy bonus to the loss:</p> <pre><code>total_loss = ppo_loss - entropy_coeff * entropy\n</code></pre> </li> <li> <p>Prevents policy from collapsing too early into deterministic behavior.</p> </li> <li> <p>Encourages ongoing exploration especially in early training.</p> </li> <li> <p>Entropy Coefficient (hyperparameter):</p> </li> <li> <p>Typically a small value (e.g. <code>0.01</code>, <code>0.001</code>)</p> </li> <li>Can be annealed (decayed) over time.</li> </ul>"},{"location":"02-entropy/#summary","title":"\ud83e\udde0 Summary","text":"<ul> <li>Entropy is a measure of uncertainty in the policy.</li> <li>Encouraging entropy helps with exploration in RL.</li> <li>PPO uses an entropy bonus to maintain a balance between exploring and exploiting.</li> </ul>"},{"location":"03-ppo/","title":"PPO","text":""},{"location":"03-ppo/#proximal-policy-optimization-ppo","title":"\ud83e\udde0 Proximal Policy Optimization (PPO)","text":""},{"location":"03-ppo/#ppo-intuition-flow","title":"\ud83d\ude80 PPO Intuition &amp; Flow","text":"<p>PPO is an online, actor-critic reinforcement learning algorithm. It builds on top of vanilla policy gradient \u2192 TRPO \u2192 and finally lands on a simpler, stable, and scalable variant called PPO.</p>"},{"location":"03-ppo/#starting-point-vanilla-policy-gradient","title":"\ud83c\udfaf Starting Point: Vanilla Policy Gradient","text":"<ul> <li> <p>Policy network: takes input state <code>s</code> and outputs a distribution over actions.</p> </li> <li> <p>Discrete space \u2192 action probs</p> </li> <li> <p>Continuous space \u2192 mean &amp; std for Gaussian</p> </li> <li> <p>Value network: takes state <code>s</code> and outputs value estimate \\(V(s)\\).   \u2192 Not Q(s, a)</p> </li> <li> <p>After rollout (sampling actions from current policy), we compute:</p> </li> </ul> \\[ \\mathcal{L}_{\\text{PG}} = -\\log(\\pi(a|s)) \\cdot A(s, a) \\] <p>Where:</p> <ul> <li>\\(A(s, a) = R - V(s)\\) is the advantage</li> <li>We want to increase likelihood of good actions (positive advantage)</li> </ul> <p>\u2139\ufe0f Minus sign is used because we minimize loss (PyTorch-style)</p>"},{"location":"03-ppo/#problem-with-vanilla-pg","title":"\u26a0\ufe0f Problem with Vanilla PG","text":"<p>If you collect a batch of experiences with one policy, then perform a large update (e.g., multiple gradient steps), \u2192 your policy can drift too far from the original behavior used to collect those rollouts. \u2192 Leads to instability and collapse.</p>"},{"location":"03-ppo/#trpo-trust-region-policy-optimization","title":"\ud83e\uddf1 TRPO: Trust Region Policy Optimization","text":"<p>TRPO introduced a trust region to limit policy changes.</p> \\[ \\max_\\theta \\; \\mathbb{E} \\left[ \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\cdot A(s, a) \\right] \\quad \\text{s.t.} \\quad \\mathbb{E}_s \\left[ \\text{KL}[\\pi_{\\theta_{\\text{old}}} \\parallel \\pi_\\theta] \\right] \\leq \\delta \\] <ul> <li>KL divergence constraint ensures policy doesn\u2019t diverge too much.</li> <li>Uses second-order optimization, conjugate gradients, Fisher matrix.</li> </ul> <p>\ud83d\udeab Hard to implement and scale!</p>"},{"location":"03-ppo/#ppo-proximal-policy-optimization","title":"\u2705 PPO: Proximal Policy Optimization","text":"<p>PPO simplifies TRPO by:</p> <ol> <li>Replacing hard KL constraint with a clipping mechanism</li> <li>(Optional) Adding KL as a penalty term, or just tracking it</li> </ol>"},{"location":"03-ppo/#ppo-clipped-objective","title":"\ud83d\udd01 PPO Clipped Objective","text":"<p>Define:</p> \\[ r_\\theta = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\] <p>Clipped loss:</p> \\[ \\mathcal{L}_{\\text{clip}} = \\mathbb{E} \\left[ \\min \\left( r_\\theta A, \\; \\text{clip}(r_\\theta, 1 - \\epsilon, 1 + \\epsilon) A \\right) \\right] \\] <ul> <li>Clipping prevents large updates by flattening the objective if the new policy moves too far from old one.</li> <li>Keeps learning stable without needing 2<sup>nd</sup>-order methods.</li> </ul>"},{"location":"03-ppo/#full-ppo-loss","title":"\ud83e\udde0 Full PPO Loss","text":"\\[ \\mathcal{L}_{\\text{PPO}} = -\\mathcal{L}_{\\text{clip}} + c_v \\cdot \\text{ValueLoss} - c_e \\cdot \\text{Entropy} \\] <p>Where:</p> <ul> <li><code>clip_loss</code>: clipped surrogate objective (stabilizes policy updates)</li> <li><code>value_loss</code>:</li> </ul> <p>$$   (V(s) - R)^2   $$</p> <p>Critic is trained to fit actual return. * <code>entropy_bonus</code>: encourages exploration</p> <p>$$   -\\sum \\pi(a|s) \\log \\pi(a|s)   $$ * Coefficients \\(c_v\\), \\(c_e\\): hyperparameters</p>"},{"location":"03-ppo/#ppo-variants-wrt-kl-divergence","title":"\ud83d\udd00 PPO Variants w.r.t KL-Divergence","text":"<ul> <li> <p>PPO-clip (common):   Only uses clipped objective. Monitors KL, but doesn\u2019t penalize it explicitly.</p> </li> <li> <p>PPO-penalty:   Adds KL divergence into the loss:</p> </li> </ul> <p>$$   \\mathcal{L} = \\mathcal{L}{\\text{clip}} - \\beta \\cdot \\text{KL}(\\pi)   $$}}, \\pi_{\\theta</p> <p>Adaptively adjusts \\(\\beta\\) if KL becomes too large or too small.</p> <ul> <li>Early stopping:   Some PPO setups just stop updates early if KL gets too big.</li> </ul>"},{"location":"03-ppo/#advantage-estimation","title":"\ud83d\udd0d Advantage Estimation","text":"<p>PPO often uses GAE (Generalized Advantage Estimation):</p> \\[ A(s, a) = \\sum_{l=0}^{T-t} (\\gamma \\lambda)^l \\cdot \\delta_{t+l} \\quad \\text{where} \\quad \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) \\] <ul> <li>Balances bias\u2013variance tradeoff with \\(\\lambda \\in [0, 1]\\)</li> </ul>"},{"location":"03-ppo/#continuous-action-space","title":"\ud83e\uddea Continuous Action Space","text":"<ul> <li>Actor network outputs mean and log_std for each action dim</li> <li>Use Normal distribution to sample action:</li> </ul> <pre><code>dist = torch.distributions.Normal(mean, std)\naction = dist.sample()\nlog_prob = dist.log_prob(action).sum(dim=-1)\nentropy = dist.entropy().sum(dim=-1)\n</code></pre>"},{"location":"03-ppo/#recap-summary","title":"\u2705 Recap Summary","text":"Component PPO Behavior Algorithm type Online, Actor-Critic Policy loss Clipped surrogate objective Value loss MSE between predicted V(s) and return Entropy bonus Encourages exploration KL divergence Optional (monitor / penalty / clip only) Exploration Via entropy; no \u03b5-greedy Stability Via clipping (instead of hard constraints)"},{"location":"03-ppo/#code","title":"Code","text":""},{"location":"03-ppo/#helper-code","title":"Helper code","text":"<pre><code>def layer_init(\n    layer: torch.nn.Module,\n    std: float = math.sqrt(2),\n    bias_const: float = 0.0,\n    ortho_init: bool = True,\n):\n    if ortho_init:\n        torch.nn.init.orthogonal_(layer.weight, std)\n        torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\n\ndef linear_annealing(optimizer: torch.optim.Optimizer, update: int, num_updates: int, initial_lr: float):\n    frac = 1.0 - (update - 1.0) / num_updates\n    lrnow = frac * initial_lr\n    for pg in optimizer.param_groups:\n        pg[\"lr\"] = lrnow\n</code></pre>"},{"location":"03-ppo/#ppo-loss-function","title":"PPO loss function","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\ndef policy_loss(advantages: torch.Tensor, ratio: torch.Tensor, clip_coef: float) -&gt; torch.Tensor:\n    pg_loss1 = -advantages * ratio\n    pg_loss2 = -advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n    return torch.max(pg_loss1, pg_loss2).mean()\n\n\ndef value_loss(\n    new_values: Tensor,\n    old_values: Tensor,\n    returns: Tensor,\n    clip_coef: float,\n    clip_vloss: bool,\n    vf_coef: float,\n) -&gt; Tensor:\n    new_values = new_values.view(-1)\n    if not clip_vloss:\n        values_pred = new_values\n    else:\n        values_pred = old_values + torch.clamp(new_values - old_values, -clip_coef, clip_coef)\n    return vf_coef * F.mse_loss(values_pred, returns)\n\n\ndef entropy_loss(entropy: Tensor, ent_coef: float) -&gt; Tensor:\n    return -entropy.mean() * ent_coef\n</code></pre>"},{"location":"03-ppo/#ppo-training-loop","title":"PPO training loop","text":"<pre><code>import math\n\nimport gymnasium as gym\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.distributions import Categorical\nfrom torchmetrics import MeanMetric\n\nfrom lightning.pytorch import LightningModule\nfrom rl.loss import entropy_loss, policy_loss, value_loss\nfrom rl.utils import layer_init\n\n\nclass PPOAgent(torch.nn.Module):\n    def __init__(self, envs: gym.vector.SyncVectorEnv, act_fun: str = \"relu\", ortho_init: bool = False) -&gt; None:\n        super().__init__()\n        if act_fun.lower() == \"relu\":\n            act_fun = torch.nn.ReLU()\n        elif act_fun.lower() == \"tanh\":\n            act_fun = torch.nn.Tanh()\n        else:\n            raise ValueError(\"Unrecognized activation function: `act_fun` must be either `relu` or `tanh`\")\n        self.critic = torch.nn.Sequential(\n            layer_init(\n                torch.nn.Linear(math.prod(envs.single_observation_space.shape), 64),\n                ortho_init=ortho_init,\n            ),\n            act_fun,\n            layer_init(torch.nn.Linear(64, 64), ortho_init=ortho_init),\n            act_fun,\n            layer_init(torch.nn.Linear(64, 1), std=1.0, ortho_init=ortho_init),\n        )\n        self.actor = torch.nn.Sequential(\n            layer_init(\n                torch.nn.Linear(math.prod(envs.single_observation_space.shape), 64),\n                ortho_init=ortho_init,\n            ),\n            act_fun,\n            layer_init(torch.nn.Linear(64, 64), ortho_init=ortho_init),\n            act_fun,\n            layer_init(torch.nn.Linear(64, envs.single_action_space.n), std=0.01, ortho_init=ortho_init),\n        )\n\n    def get_action(self, x: Tensor, action: Tensor = None) -&gt; tuple[Tensor, Tensor, Tensor]:\n        logits = self.actor(x)\n        distribution = Categorical(logits=logits)\n        if action is None:\n            action = distribution.sample()\n        return action, distribution.log_prob(action), distribution.entropy()\n\n    def get_greedy_action(self, x: Tensor) -&gt; Tensor:\n        logits = self.actor(x)\n        probs = F.softmax(logits, dim=-1)\n        return torch.argmax(probs, dim=-1)\n\n    def get_value(self, x: Tensor) -&gt; Tensor:\n        return self.critic(x)\n\n    def get_action_and_value(self, x: Tensor, action: Tensor = None) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]:\n        action, log_prob, entropy = self.get_action(x, action)\n        value = self.get_value(x)\n        return action, log_prob, entropy, value\n\n    def forward(self, x: Tensor, action: Tensor = None) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]:\n        return self.get_action_and_value(x, action)\n\n    @torch.no_grad()\n    def estimate_returns_and_advantages(\n        self,\n        rewards: Tensor,\n        values: Tensor,\n        dones: Tensor,\n        next_obs: Tensor,\n        next_done: Tensor,\n        num_steps: int,\n        gamma: float,\n        gae_lambda: float,\n    ) -&gt; tuple[Tensor, Tensor]:\n        next_value = self.get_value(next_obs).reshape(1, -1)\n        advantages = torch.zeros_like(rewards)\n        lastgaelam = 0\n        for t in reversed(range(num_steps)):\n            if t == num_steps - 1:\n                nextnonterminal = torch.logical_not(next_done)\n                nextvalues = next_value\n            else:\n                nextnonterminal = torch.logical_not(dones[t + 1])\n                nextvalues = values[t + 1]\n            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n        returns = advantages + values\n        return returns, advantages\n\n\nclass PPOLightningAgent(LightningModule):\n    def __init__(\n        self,\n        envs: gym.vector.SyncVectorEnv,\n        act_fun: str = \"relu\",\n        ortho_init: bool = False,\n        vf_coef: float = 1.0,\n        ent_coef: float = 0.0,\n        clip_coef: float = 0.2,\n        clip_vloss: bool = False,\n        normalize_advantages: bool = False,\n        **torchmetrics_kwargs,\n    ):\n        super().__init__()\n        if act_fun.lower() == \"relu\":\n            act_fun = torch.nn.ReLU()\n        elif act_fun.lower() == \"tanh\":\n            act_fun = torch.nn.Tanh()\n        else:\n            raise ValueError(\"Unrecognized activation function: `act_fun` must be either `relu` or `tanh`\")\n        self.vf_coef = vf_coef\n        self.ent_coef = ent_coef\n        self.clip_coef = clip_coef\n        self.clip_vloss = clip_vloss\n        self.normalize_advantages = normalize_advantages\n        self.critic = torch.nn.Sequential(\n            layer_init(\n                torch.nn.Linear(math.prod(envs.single_observation_space.shape), 64),\n                ortho_init=ortho_init,\n            ),\n            act_fun,\n            layer_init(torch.nn.Linear(64, 64), ortho_init=ortho_init),\n            act_fun,\n            layer_init(torch.nn.Linear(64, 1), std=1.0, ortho_init=ortho_init),\n        )\n        self.actor = torch.nn.Sequential(\n            layer_init(\n                torch.nn.Linear(math.prod(envs.single_observation_space.shape), 64),\n                ortho_init=ortho_init,\n            ),\n            act_fun,\n            layer_init(torch.nn.Linear(64, 64), ortho_init=ortho_init),\n            act_fun,\n            layer_init(torch.nn.Linear(64, envs.single_action_space.n), std=0.01, ortho_init=ortho_init),\n        )\n        self.avg_pg_loss = MeanMetric(**torchmetrics_kwargs)\n        self.avg_value_loss = MeanMetric(**torchmetrics_kwargs)\n        self.avg_ent_loss = MeanMetric(**torchmetrics_kwargs)\n\n    def get_action(self, x: Tensor, action: Tensor = None) -&gt; tuple[Tensor, Tensor, Tensor]:\n        logits = self.actor(x)\n        distribution = Categorical(logits=logits)\n        if action is None:\n            action = distribution.sample()\n        return action, distribution.log_prob(action), distribution.entropy()\n\n    def get_greedy_action(self, x: Tensor) -&gt; Tensor:\n        logits = self.actor(x)\n        probs = F.softmax(logits, dim=-1)\n        return torch.argmax(probs, dim=-1)\n\n    def get_value(self, x: Tensor) -&gt; Tensor:\n        return self.critic(x)\n\n    def get_action_and_value(self, x: Tensor, action: Tensor = None) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]:\n        action, log_prob, entropy = self.get_action(x, action)\n        value = self.get_value(x)\n        return action, log_prob, entropy, value\n\n    def forward(self, x: Tensor, action: Tensor = None) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]:\n        return self.get_action_and_value(x, action)\n\n    @torch.no_grad()\n    def estimate_returns_and_advantages(\n        self,\n        rewards: Tensor,\n        values: Tensor,\n        dones: Tensor,\n        next_obs: Tensor,\n        next_done: Tensor,\n        num_steps: int,\n        gamma: float,\n        gae_lambda: float,\n    ) -&gt; tuple[Tensor, Tensor]:\n        next_value = self.get_value(next_obs).reshape(1, -1)\n        advantages = torch.zeros_like(rewards)\n        lastgaelam = 0\n        for t in reversed(range(num_steps)):\n            if t == num_steps - 1:\n                nextnonterminal = torch.logical_not(next_done)\n                nextvalues = next_value\n            else:\n                nextnonterminal = torch.logical_not(dones[t + 1])\n                nextvalues = values[t + 1]\n            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n        returns = advantages + values\n        return returns, advantages\n\n    def training_step(self, batch: dict[str, Tensor]):\n        # Get actions and values given the current observations\n        _, newlogprob, entropy, newvalue = self(batch[\"obs\"], batch[\"actions\"].long())\n        logratio = newlogprob - batch[\"logprobs\"]\n        ratio = logratio.exp()\n\n        # Policy loss\n        advantages = batch[\"advantages\"]\n        if self.normalize_advantages:\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        pg_loss = policy_loss(batch[\"advantages\"], ratio, self.clip_coef)\n\n        # Value loss\n        v_loss = value_loss(\n            newvalue,\n            batch[\"values\"],\n            batch[\"returns\"],\n            self.clip_coef,\n            self.clip_vloss,\n            self.vf_coef,\n        )\n\n        # Entropy loss\n        ent_loss = entropy_loss(entropy, self.ent_coef)\n\n        # Update metrics\n        self.avg_pg_loss(pg_loss)\n        self.avg_value_loss(v_loss)\n        self.avg_ent_loss(ent_loss)\n\n        # Overall loss\n        return pg_loss + ent_loss + v_loss\n\n    def on_train_epoch_end(self, global_step: int) -&gt; None:\n        # Log metrics and reset their internal state\n        self.logger.log_metrics(\n            {\n                \"Loss/policy_loss\": self.avg_pg_loss.compute(),\n                \"Loss/value_loss\": self.avg_value_loss.compute(),\n                \"Loss/entropy_loss\": self.avg_ent_loss.compute(),\n            },\n            global_step,\n        )\n        self.reset_metrics()\n\n    def reset_metrics(self):\n        self.avg_pg_loss.reset()\n        self.avg_value_loss.reset()\n        self.avg_ent_loss.reset()\n\n    def configure_optimizers(self, lr: float):\n        return torch.optim.Adam(self.parameters(), lr=lr, eps=1e-4)\n</code></pre> <ul> <li>Above code has been taken from <code>lightning-ai/pytorch-lightning/examples</code> repository.</li> </ul>"},{"location":"03-ppo/#summary","title":"\ud83d\udcd6 Summary","text":"<p>overview</p> <ul> <li>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that trains an agent to learn good behavior through interaction with its environment.</li> <li>It uses two neural networks: one to decide actions (the policy network) and another to estimate how good a state is (the value network).</li> <li>After collecting a batch of experiences by running the current policy, PPO computes how much better or worse the chosen actions were compared to what was expected (advantage).</li> <li>It then updates the policy in a way that improves it, but not too drastically \u2014 ensuring stability by limiting how much the new policy can diverge from the old one.</li> <li>To encourage exploration, it adds an entropy bonus, and it adjusts the learning rate gradually over time (linear annealing).</li> <li>PPO balances simplicity, performance, and stability, making it one of the most widely used algorithms in modern reinforcement learning.</li> </ul>"},{"location":"04-dpo/","title":"Direct Preference Optimization (DPO)","text":"<p>Your language model is secretly a Reward Model.</p>"},{"location":"04-dpo/#formula","title":"Formula:","text":"\\[ \\mathcal{L}_{DPO} = - \\mathbb{E}_{x, y_w, y_l} \\left[ \\log \\sigma \\left(\\beta \\log \\frac{\\pi\\_\\theta(y_w|x)}{\\pi\\_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi\\_\\theta(y_l|x)}{\\pi\\_{ref}(y_l|x)} \\right)\\right] \\] <p>the above loss function when being implemented replaces <code>division with subtraction</code>:</p> \\[ \\mathcal{L}_{DPO} = - \\mathbb{E}_{x, y_w, y_l} \\left[ \\log \\sigma \\left( \\beta (\\log \\pi\\_\\theta(y_w|x) - \\log \\pi\\_{ref}(y_w|x)) - \\beta (\\log \\pi\\_\\theta(y_l|x) - \\log \\pi\\_{ref}(y_l|x)) \\right)\\right] \\]"},{"location":"04-dpo/#log-sigmoid","title":"\ud83d\ude09 log sigmoid","text":"\\[ LogSigmoid (X) = \\log \\sigma(x) = \\log \\left( \\frac{1}{1 + e^{-x}} \\right ) \\]"},{"location":"04-dpo/#code","title":"Code","text":"<pre><code>import torch.nn.functional as F\n\ndef dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):\n    \"\"\"\n    pi_logps: policy logprobs, shape (B,)\n    ref_logps: reference model logprobs, shape (B,)\n    yw_idxs: preferred completion indices in [0, B-1], shape (T,)\n    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)\n    beta: temperature controlling strength of KL penalty\n    Each pair of (yw_idxs[i], yl_idxs[i]) represents the\n    indices of a single preference pair.\n    \"\"\"\n    pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]\n    ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]\n    pi_logratios = pi_yw_logps - pi_yl_logps\n    ref_logratios = ref_yw_logps - ref_yl_logps\n    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))\n    rewards = beta * (pi_logps - ref_logps).detach()\n    return losses, rewards\n</code></pre>"},{"location":"04-dpo/#direct-preference-optimization-dpo_1","title":"\ud83e\udde0 Direct Preference Optimization (DPO)","text":"<p>DPO is a method to align language models without reinforcement learning or training a reward model. Instead of generating a numeric reward signal, we use human preferences directly.</p>"},{"location":"04-dpo/#setup","title":"\ud83e\udea2 Setup","text":"<p>We are given a dataset of:</p> <pre><code>(prompt, accepted_response, rejected_response)\n=&gt; (x, y_w, y_l)\n</code></pre> <p>The goal is to fine-tune a policy (language model) to prefer <code>y_w</code> over <code>y_l</code>.</p>"},{"location":"04-dpo/#core-idea","title":"\ud83c\udfd7\ufe0f Core Idea","text":"<p>Instead of RL or reward modeling, DPO fine-tunes a model to maximize the preference likelihood:</p> <p>The model should assign higher probability to <code>y_w</code> than to <code>y_l</code>.</p> <p>We use a reference model (<code>\u03c0_ref</code>) \u2014 typically the base model \u2014 which is frozen. We compare the fine-tuned model (<code>\u03c0_\u03b8</code>) against this.</p>"},{"location":"04-dpo/#log-probability-computation","title":"\ud83d\udd0d Log Probability Computation","text":"<p>For each prompt-response pair:</p> <pre><code>log_pi_theta_yw = log \u03c0_\u03b8(y_w | x)   # logprob of accepted response\nlog_pi_theta_yl = log \u03c0_\u03b8(y_l | x)   # logprob of rejected response\n\nlog_pi_ref_yw = log \u03c0_ref(y_w | x)\nlog_pi_ref_yl = log \u03c0_ref(y_l | x)\n</code></pre> <p>We usually compute <code>log \u03c0(y|x)</code> by summing the log probabilities of each token in the response.</p>"},{"location":"04-dpo/#loss-function","title":"\ud83e\uddee Loss Function","text":"<p>DPO loss:</p> \\[ \\mathcal{L}_{DPO} = - \\mathbb{E}_{x, y_w, y_l} \\left[ \\log \\sigma \\left( \\beta \\cdot \\left( \\log \\frac{\\pi\\_\\theta(y_w|x)}{\\pi\\_\\text{ref}(y_w|x)} - \\log \\frac{\\pi\\_\\theta(y_l|x)}{\\pi\\_\\text{ref}(y_l|x)} \\right) \\right) \\right] \\] <p>Using log identities:</p> \\[ = - \\log \\sigma \\left( \\beta \\cdot \\left[ (\\log \u03c0_\u03b8(y_w) - \\log \u03c0_\u03b8(y_l)) - (\\log \u03c0_ref(y_w) - \\log \u03c0_ref(y_l)) \\right] \\right) \\]"},{"location":"04-dpo/#why-logsigmoid","title":"\ud83d\udce6 Why LogSigmoid?","text":"<p>We use:</p> \\[ \\log \\sigma(z) = \\log \\left( \\frac{1}{1 + e^{-z}} \\right) \\]"},{"location":"04-dpo/#intuition","title":"Intuition:","text":"<ul> <li>It acts like a binary preference classifier.</li> <li>Encourages model to rank <code>y_w</code> over <code>y_l</code>.</li> <li>Smooth, differentiable, and stable even for large <code>z</code>.</li> <li>Equivalent to maximizing the log-likelihood of choosing the better output.</li> </ul> <p>When:</p> <ul> <li><code>\u0394 \u226b 0</code>: \u2192 <code>logsigmoid</code> \u2248 0 \u2192 \u2705 low loss (model prefers <code>y_w</code>)</li> <li><code>\u0394 \u226a 0</code>: \u2192 <code>logsigmoid</code> \u226a 0 \u2192 \u274c high loss (model prefers <code>y_l</code>)</li> </ul>"},{"location":"04-dpo/#code_1","title":"\ud83d\udcbb Code","text":"<pre><code>import torch.nn.functional as F\n\ndef dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):\n    \"\"\"\n    pi_logps: model logprobs, shape (B,)\n    ref_logps: reference model logprobs, shape (B,)\n    yw_idxs, yl_idxs: indices for chosen and rejected responses\n    beta: scaling factor (temperature)\n    \"\"\"\n    pi_yw_logps = pi_logps[yw_idxs]\n    pi_yl_logps = pi_logps[yl_idxs]\n    ref_yw_logps = ref_logps[yw_idxs]\n    ref_yl_logps = ref_logps[yl_idxs]\n\n    pi_logratios = pi_yw_logps - pi_yl_logps\n    ref_logratios = ref_yw_logps - ref_yl_logps\n\n    loss = -F.logsigmoid(beta * (pi_logratios - ref_logratios))\n\n    # Optional: reward signal for logging or auxiliary losses\n    rewards = beta * (pi_logps - ref_logps).detach()\n\n    return loss, rewards\n</code></pre>"},{"location":"04-dpo/#notes","title":"\u26a0\ufe0f Notes","text":"<ul> <li>DPO doesn't use a separate reward model.</li> <li>No explicit KL-divergence term like in PPO \u2014 but the log-ratio implicitly acts like a KL regularizer.</li> <li>You typically sum logprobs over the entire sequence to get <code>log \u03c0(y | x)</code>.</li> <li>DPO is much easier to train than PPO, and doesn\u2019t need rollouts or reward shaping.</li> </ul>"},{"location":"04-dpo/#variants-flexibility","title":"\ud83d\udd04 Variants / Flexibility","text":"<ul> <li>Some variants skip the <code>\u03c0_ref</code> term, especially when no base model is available.</li> <li><code>\u03b2</code> controls the strength of alignment \u2014 higher \u03b2 pushes stronger preference.</li> </ul>"},{"location":"04-dpo/#final-summary","title":"\ud83c\udfc1 Final Summary","text":"<ul> <li>DPO = simple, stable, and effective fine-tuning method using preference data.</li> <li>Just needs a frozen base model and a set of chosen vs rejected completions.</li> <li>It's a clean way to align LLMs without the full complexity of RLHF.</li> </ul>"},{"location":"04-dpo/#how-is-language-model-aligned","title":"How is language model aligned?","text":"<ul> <li>We input prompt into the model, and it yields the probability distribution over the next token.</li> <li>For each accepted and rejected response, we get the probability of the next token and take its log.</li> <li>We then input <code>prompt+next token</code> into the model, and repeat the process, to get the probability distribution over the next token.</li> <li>We continue this until we reach the end of the response.</li> <li>Finally, we sum the log probabilities of all tokens in the response to get the log probability of the entire response to receive <code>log \u03c0(y | x)</code>.</li> </ul> <p>we sum the probability of next tokens and then take the log, we take the log of each next token probability and then sum?</p> <p>In DPO (and most language modeling tasks), we take the log of each token's probability and then sum them.</p>"},{"location":"04-dpo/#why","title":"Why?","text":"<p>Because the probability of the entire sequence is the product of individual token probabilities:</p> \\[ P(y \\mid x) = P(y_1 \\mid x) \\cdot P(y_2 \\mid x, y_1) \\cdot P(y_3 \\mid x, y_1, y_2) \\cdots \\] <p>Taking the log:</p> \\[ \\log P(y \\mid x) = \\log P(y_1 \\mid x) + \\log P(y_2 \\mid x, y_1) + \\log P(y_3 \\mid x, y_1, y_2) + \\cdots \\] <p>So, you sum log-probabilities per token to get the sequence-level log-probability.</p>"},{"location":"05-grpo/","title":"\ud83e\udde0 GRPO: Group Relative Policy Optimization","text":"<p>GRPO Overview</p> <p>my understanding of grpo is that it's very similar to ppo. Some changes are: it reintroduces explicit kl divergence term for loss, and replaces value model and instead rather than generating one response, it uses beam search to get K responses, and the group of K response is ranked relatively</p> <ul> <li>advantage of taken action a_i = (reward for a_i - mean reward for all the action(output))/std dev</li> </ul> <p>rest all seems similar.</p>"},{"location":"05-grpo/#what-is-grpo","title":"\ud83d\udd0d What is GRPO?","text":"<p>GRPO (Group Relative Policy Optimization) is an RLHF (Reinforcement Learning from Human Feedback) method inspired by PPO (Proximal Policy Optimization), designed to fine-tune large language models (LLMs) using relative ranking feedback within a group of outputs, instead of absolute scores or pairwise preferences.</p> <p>Unlike PPO:</p> <ul> <li>GRPO does not use a value model.</li> <li>It uses beam search to generate multiple completions (responses) per prompt.</li> <li>It calculates relative advantages within each group of completions.</li> <li>It often includes an explicit KL divergence term in the loss.</li> </ul>"},{"location":"05-grpo/#grpo-training-pipeline","title":"\ud83c\udfd7\ufe0f GRPO Training Pipeline","text":""},{"location":"05-grpo/#1-pretrained-reward-model","title":"1. Pretrained Reward Model","text":"<ul> <li>Train a reward model from preference data (like in DPO/PPO).</li> <li>Reward model maps a (prompt, completion) pair to a scalar score.</li> </ul>"},{"location":"05-grpo/#2-prompt-beam-search-k-completions","title":"2. Prompt \u2192 Beam Search \u2192 K Completions","text":"<ul> <li>For each prompt, run beam search to generate K candidate completions.</li> <li>These K completions form a group.</li> </ul>"},{"location":"05-grpo/#3-score-group-completions","title":"3. Score Group Completions","text":"<ul> <li>For each generated response \\(y_i\\) in group \\(G\\), compute:</li> </ul> <p>$$   R(y_i) = \\text{reward model output}   $$</p>"},{"location":"05-grpo/#4-groupwise-advantage-calculation","title":"4. Groupwise Advantage Calculation","text":"<p>For each response in group \\(G\\):</p> \\[ A_i = \\frac{R(y_i) - \\mu_G}{\\sigma_G} \\] <p>Where:</p> <ul> <li>\\(\\mu_G\\) is the mean reward in group \\(G\\)</li> <li>\\(\\sigma_G\\) is the standard deviation</li> </ul> <p>This normalizes rewards and allows the model to focus on relative improvement.</p>"},{"location":"05-grpo/#5-ppo-style-policy-update","title":"5. PPO-style Policy Update","text":"<p>Use PPO\u2019s clipped surrogate objective:</p> \\[ L^{CLIP}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r\\_\\theta A_i, \\text{clip}(r\\_\\theta, 1-\\epsilon, 1+\\epsilon) A_i \\right) \\right] \\] <p>Where:</p> <ul> <li>\\(r\\_\\theta = \\frac{\\pi\\_\\theta(y_i)}{\\pi\\_{\\text{old}}(y_i)}\\)</li> <li>\\(\\pi\\_\\theta\\): updated policy</li> <li>\\(\\pi\\_{\\text{old}}\\): policy before update</li> </ul>"},{"location":"05-grpo/#6-optional-kl-penalty","title":"6. Optional KL Penalty","text":"<p>Some GRPO variants include an explicit KL divergence penalty:</p> \\[ L\\_{KL} = \\beta \\cdot \\text{KL}[\\pi\\_\\theta || \\pi\\_{\\text{ref}}] \\] <p>Total Loss:</p> \\[ L\\_{total} = L^{CLIP} + L\\_{KL} \\]"},{"location":"05-grpo/#loss","title":"Loss","text":"<p>GRPO operates over groups of responses (like beams from beam search), so the loss is summed over all elements in the group.</p> <p>Here\u2019s how it works conceptually:</p>"},{"location":"05-grpo/#group-wise-computation-in-grpo","title":"\u2705 Group-wise Computation in GRPO","text":"<p>For each prompt \\(x\\), you generate a group of responses \\({y_1, y_2, ..., y_K}\\) using beam search.</p> <p>Then:</p> <ol> <li> <p>Get rewards:    Use a reward model to get scores \\({r_1, r_2, ..., r_K}\\) for each \\(y_i\\)</p> </li> <li> <p>Normalize rewards within the group:    This gives you relative advantages:</p> </li> </ol> \\[ A_i = \\frac{r_i - \\mu_r}{\\sigma_r + \\epsilon} \\] <ol> <li> <p>Compute log-probabilities:</p> </li> <li> <p>\\(\\log \\pi\\_\\theta(y_i|x)\\): current policy</p> </li> <li> <p>\\(\\log \\pi\\_{\\text{ref}}(y_i|x)\\): reference policy (often frozen)</p> </li> <li> <p>Compute surrogate objective:    For each \\(y_i\\) in the group, compute the PPO-style clipped term with the normalized reward as the advantage:</p> </li> </ol> \\[ \\text{loss}_i = - \\min \\left( r_i \\cdot \\frac{\\pi_\\theta(y_i|x)}{\\pi\\_{\\text{ref}}(y_i|x)}, \\text{clip}\\left(\\frac{\\pi\\_\\theta(y_i|x)}{\\pi\\_{\\text{ref}}(y_i|x)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\cdot r_i \\right) \\] <ol> <li>Sum over the group:    The total loss for one prompt is:</li> </ol> \\[ \\mathcal{L}_{\\text{GRPO}}^{(x)} = \\sum_{i=1}^{K} \\text{loss}\\_i \\] <ol> <li>Average over batch:    Final training loss is averaged across all prompts in the batch.</li> </ol> <p>So yes \u2014 GRPO loss aggregates over the group for each prompt to respect relative rankings rather than treating responses independently. This is what gives it the \u201cgroup relative\u201d flavor, unlike PPO which considers each sample in isolation.</p>"},{"location":"05-grpo/#key-differences-from-ppo","title":"\ud83e\uddea Key Differences from PPO","text":"Aspect PPO GRPO Value Model Required Not used Response Sampling 1 response per prompt K responses via beam search Advantage Estimate GAE or MC return Relative group-wise advantage Reward Source Return or reward model Reward model (pretrained) KL Term Optional (implicit or explicit) Often explicitly added"},{"location":"05-grpo/#why-group-normalization","title":"\ud83e\udde0 Why Group Normalization?","text":"<ul> <li>Helps focus learning on relative preference signals, rather than noisy reward scores.</li> <li>Avoids need for value model or full trajectories.</li> <li>Robust to scale shifts in reward model outputs.</li> </ul>"},{"location":"05-grpo/#where-grpo-shines","title":"\ud83e\udd16 Where GRPO Shines","text":"<ul> <li>When you want to rank multiple outputs for hard reasoning tasks.</li> <li>When training on multi-step reasoning questions, where final answer is too sparse as signal.</li> <li>When comparing several candidate generations is easier than labeling absolute scores or winning pairs.</li> </ul>"},{"location":"05-grpo/#origin","title":"\ud83d\udcda Origin","text":"<p>GRPO was introduced in:</p> <p>\"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\" (2024)</p>"},{"location":"05-grpo/#summary","title":"\u2705 Summary","text":"<p>GRPO is:</p> <ul> <li>A clean generalization of PPO for grouped outputs.</li> <li>Practical for reasoning tasks where multiple candidates are easier to evaluate relatively.</li> <li>Easier to train than full-on value-based PPO.</li> </ul>"}]}